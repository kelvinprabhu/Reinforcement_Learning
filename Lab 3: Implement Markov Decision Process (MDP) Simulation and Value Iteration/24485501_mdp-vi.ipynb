{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f1622eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35142e5",
   "metadata": {},
   "source": [
    "Markov Decision Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55a9ea69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP:\n",
    "    \"\"\"Markov Decision Process Environment\"\"\"\n",
    "    \n",
    "    def __init__(self, env_type='grid'):\n",
    "        self.environments = {\n",
    "            'grid': {\n",
    "                'name': 'Grid World MDP',\n",
    "                'size': 4,\n",
    "                'terminals': [(0, 0), (3, 3)],\n",
    "                'obstacles': [(1, 1)],\n",
    "                'rewards': {\n",
    "                    (0, 0): 0,      # Terminal state\n",
    "                    (3, 3): 1,      # Terminal state (goal)\n",
    "                    (1, 1): -1,     # Obstacle\n",
    "                    'default': -0.04  # Living penalty\n",
    "                },\n",
    "                'transition_prob': 0.8  # Probability of intended action\n",
    "            },\n",
    "            'cliff': {\n",
    "                'name': 'Cliff Walking MDP',\n",
    "                'size': (4, 12),\n",
    "                'terminals': [(3, 11)],\n",
    "                'obstacles': [(3, i) for i in range(1, 11)],  # The cliff\n",
    "                'rewards': {\n",
    "                    (3, 11): 1,     # Goal\n",
    "                    **{(3, i): -1 for i in range(1, 11)},  # Cliff penalties\n",
    "                    'default': -0.01\n",
    "                },\n",
    "                'transition_prob': 1.0  # Deterministic for cliff\n",
    "            },\n",
    "            'stochastic': {\n",
    "                'name': 'Stochastic Grid MDP',\n",
    "                'size': 5,\n",
    "                'terminals': [(0, 4), (4, 4)],\n",
    "                'obstacles': [(2, 2)],\n",
    "                'rewards': {\n",
    "                    (0, 4): -1,     # Bad terminal\n",
    "                    (4, 4): 1,      # Good terminal\n",
    "                    (2, 2): -0.5,   # Obstacle\n",
    "                    'default': -0.02\n",
    "                },\n",
    "                'transition_prob': 0.7  # More stochastic\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        config = self.environments[env_type]\n",
    "        self.name = config['name']\n",
    "        \n",
    "        # Handle both square and rectangular grids\n",
    "        if isinstance(config['size'], tuple):\n",
    "            self.rows, self.cols = config['size']\n",
    "        else:\n",
    "            self.rows = self.cols = config['size']\n",
    "        \n",
    "        self.terminals = config['terminals']\n",
    "        self.obstacles = config['obstacles']\n",
    "        self.rewards = config['rewards']\n",
    "        self.transition_prob = config['transition_prob']\n",
    "        \n",
    "        # Actions: up, down, left, right\n",
    "        self.actions = ['↑', '↓', '←', '→']\n",
    "        self.action_effects = {\n",
    "            '↑': (-1, 0),\n",
    "            '↓': (1, 0),\n",
    "            '←': (0, -1),\n",
    "            '→': (0, 1)\n",
    "        }\n",
    "        \n",
    "        # Get all valid states\n",
    "        self.states = []\n",
    "        for i in range(self.rows):\n",
    "            for j in range(self.cols):\n",
    "                if (i, j) not in self.obstacles:\n",
    "                    self.states.append((i, j))\n",
    "    \n",
    "    def get_reward(self, state):\n",
    "        \"\"\"Get reward for a state\"\"\"\n",
    "        if state in self.rewards:\n",
    "            return self.rewards[state]\n",
    "        return self.rewards['default']\n",
    "    \n",
    "    def is_terminal(self, state):\n",
    "        \"\"\"Check if state is terminal\"\"\"\n",
    "        return state in self.terminals\n",
    "    \n",
    "    def get_next_state(self, state, action):\n",
    "        \"\"\"Get next state given current state and action\"\"\"\n",
    "        if self.is_terminal(state):\n",
    "            return state\n",
    "        \n",
    "        effect = self.action_effects[action]\n",
    "        next_state = (state[0] + effect[0], state[1] + effect[1])\n",
    "        \n",
    "        # Check boundaries and obstacles\n",
    "        if (0 <= next_state[0] < self.rows and \n",
    "            0 <= next_state[1] < self.cols and\n",
    "            next_state not in self.obstacles):\n",
    "            return next_state\n",
    "        return state  # Stay in place if invalid\n",
    "    \n",
    "    def get_transition_prob(self, state, action, next_state):\n",
    "        \"\"\"Get transition probability P(s'|s,a)\"\"\"\n",
    "        if self.is_terminal(state):\n",
    "            return 1.0 if next_state == state else 0.0\n",
    "        \n",
    "        intended_next = self.get_next_state(state, action)\n",
    "        \n",
    "        if self.transition_prob == 1.0:  # Deterministic\n",
    "            return 1.0 if next_state == intended_next else 0.0\n",
    "        \n",
    "        # Stochastic: might slip to perpendicular directions\n",
    "        if next_state == intended_next:\n",
    "            return self.transition_prob\n",
    "        \n",
    "        # Calculate perpendicular actions\n",
    "        perpendicular_actions = []\n",
    "        if action in ['↑', '↓']:\n",
    "            perpendicular_actions = ['←', '→']\n",
    "        else:\n",
    "            perpendicular_actions = ['↑', '↓']\n",
    "        \n",
    "        # Check if next_state is reachable via perpendicular slip\n",
    "        for perp_action in perpendicular_actions:\n",
    "            if self.get_next_state(state, perp_action) == next_state:\n",
    "                return (1 - self.transition_prob) / 2\n",
    "        \n",
    "        return 0.0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f85ebe7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyIteration:\n",
    "    \"\"\"Policy Iteration Algorithm for MDP\"\"\"\n",
    "    \n",
    "    def __init__(self, mdp, gamma=0.9, theta=1e-6):\n",
    "        self.mdp = mdp\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.theta = theta  # Convergence threshold\n",
    "        \n",
    "        # Initialize random policy\n",
    "        self.policy = {}\n",
    "        for state in mdp.states:\n",
    "            if not mdp.is_terminal(state):\n",
    "                self.policy[state] = np.random.choice(mdp.actions)\n",
    "            else:\n",
    "                self.policy[state] = None\n",
    "        \n",
    "        # Initialize value function\n",
    "        self.V = {state: 0.0 for state in mdp.states}\n",
    "        \n",
    "        # Track iterations\n",
    "        self.eval_iterations = []\n",
    "        self.improvement_count = 0\n",
    "        self.value_history = []\n",
    "        self.policy_history = []\n",
    "    \n",
    "    def policy_evaluation(self, max_iterations=1000):\n",
    "        \"\"\"Evaluate current policy\"\"\"\n",
    "        iteration = 0\n",
    "        \n",
    "        for iteration in range(max_iterations):\n",
    "            delta = 0\n",
    "            new_V = self.V.copy()\n",
    "            \n",
    "            for state in self.mdp.states:\n",
    "                if self.mdp.is_terminal(state):\n",
    "                    continue\n",
    "                \n",
    "                action = self.policy[state]\n",
    "                v = 0\n",
    "                \n",
    "                # Calculate expected value\n",
    "                for next_state in self.mdp.states:\n",
    "                    prob = self.mdp.get_transition_prob(state, action, next_state)\n",
    "                    reward = self.mdp.get_reward(next_state)\n",
    "                    v += prob * (reward + self.gamma * self.V[next_state])\n",
    "                \n",
    "                new_V[state] = v\n",
    "                delta = max(delta, abs(v - self.V[state]))\n",
    "            \n",
    "            self.V = new_V\n",
    "            \n",
    "            if delta < self.theta:\n",
    "                break\n",
    "        \n",
    "        self.eval_iterations.append(iteration + 1)\n",
    "        return iteration + 1\n",
    "    \n",
    "    def policy_improvement(self):\n",
    "        \"\"\"Improve policy based on current value function\"\"\"\n",
    "        policy_stable = True\n",
    "        \n",
    "        for state in self.mdp.states:\n",
    "            if self.mdp.is_terminal(state):\n",
    "                continue\n",
    "            \n",
    "            old_action = self.policy[state]\n",
    "            \n",
    "            # Find best action\n",
    "            action_values = {}\n",
    "            for action in self.mdp.actions:\n",
    "                q_value = 0\n",
    "                for next_state in self.mdp.states:\n",
    "                    prob = self.mdp.get_transition_prob(state, action, next_state)\n",
    "                    reward = self.mdp.get_reward(next_state)\n",
    "                    q_value += prob * (reward + self.gamma * self.V[next_state])\n",
    "                action_values[action] = q_value\n",
    "            \n",
    "            # Select best action (greedy)\n",
    "            best_action = max(action_values, key=action_values.get)\n",
    "            self.policy[state] = best_action\n",
    "            \n",
    "            if old_action != best_action:\n",
    "                policy_stable = False\n",
    "        \n",
    "        return policy_stable\n",
    "    \n",
    "    def iterate(self, max_iterations=100):\n",
    "        \"\"\"Run policy iteration until convergence\"\"\"\n",
    "        for i in range(max_iterations):\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Policy Iteration {i + 1}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            # Policy Evaluation\n",
    "            print(\"Evaluating policy...\")\n",
    "            eval_iters = self.policy_evaluation()\n",
    "            print(f\"  Converged in {eval_iters} iterations\")\n",
    "            \n",
    "            # Store history\n",
    "            self.value_history.append(self.V.copy())\n",
    "            self.policy_history.append(self.policy.copy())\n",
    "            \n",
    "            # Policy Improvement\n",
    "            print(\"Improving policy...\")\n",
    "            policy_stable = self.policy_improvement()\n",
    "            self.improvement_count += 1\n",
    "            \n",
    "            if policy_stable:\n",
    "                print(f\"\\n✓ Policy converged after {i + 1} iterations!\")\n",
    "                break\n",
    "        \n",
    "        return self.policy, self.V\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1432297a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyIterationVisualizer:\n",
    "    \"\"\"Visualization for Policy Iteration\"\"\"\n",
    "    \n",
    "    def __init__(self, mdp, policy_iter):\n",
    "        self.mdp = mdp\n",
    "        self.policy_iter = policy_iter\n",
    "        \n",
    "    def visualize_results(self):\n",
    "        \"\"\"Create comprehensive visualization\"\"\"\n",
    "        fig = plt.figure(figsize=(16, 10))\n",
    "        gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "        \n",
    "        # Title\n",
    "        fig.suptitle(f'Policy Iteration for {self.mdp.name}', \n",
    "                    fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # 1. Final Policy\n",
    "        ax1 = fig.add_subplot(gs[0, 0])\n",
    "        self.draw_policy(ax1, self.policy_iter.policy, 'Final Optimal Policy')\n",
    "        \n",
    "        # 2. Value Function Heatmap\n",
    "        ax2 = fig.add_subplot(gs[0, 1])\n",
    "        self.draw_value_function(ax2, self.policy_iter.V, 'State Value Function')\n",
    "        \n",
    "        # 3. Statistics\n",
    "        ax3 = fig.add_subplot(gs[0, 2])\n",
    "        self.draw_statistics(ax3)\n",
    "        \n",
    "        # 4. Policy Evolution (first few iterations)\n",
    "        iterations_to_show = min(3, len(self.policy_iter.policy_history))\n",
    "        for i in range(iterations_to_show):\n",
    "            ax = fig.add_subplot(gs[1, i])\n",
    "            self.draw_policy(ax, self.policy_iter.policy_history[i], \n",
    "                           f'Policy Iteration {i+1}')\n",
    "        \n",
    "        # 5. Value Evolution\n",
    "        for i in range(iterations_to_show):\n",
    "            ax = fig.add_subplot(gs[2, i])\n",
    "            self.draw_value_function(ax, self.policy_iter.value_history[i],\n",
    "                                   f'Values Iteration {i+1}')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def draw_policy(self, ax, policy, title):\n",
    "        \"\"\"Draw policy arrows on grid\"\"\"\n",
    "        ax.set_title(title, fontsize=11, fontweight='bold')\n",
    "        ax.set_xlim(0, self.mdp.cols)\n",
    "        ax.set_ylim(0, self.mdp.rows)\n",
    "        ax.set_aspect('equal')\n",
    "        ax.invert_yaxis()\n",
    "        \n",
    "        # Draw grid\n",
    "        for i in range(self.mdp.rows + 1):\n",
    "            ax.axhline(i, color='gray', linewidth=0.5)\n",
    "        for j in range(self.mdp.cols + 1):\n",
    "            ax.axvline(j, color='gray', linewidth=0.5)\n",
    "        \n",
    "        # Draw cells\n",
    "        for i in range(self.mdp.rows):\n",
    "            for j in range(self.mdp.cols):\n",
    "                state = (i, j)\n",
    "                \n",
    "                # Terminals\n",
    "                if state in self.mdp.terminals:\n",
    "                    color = 'lightgreen' if self.mdp.get_reward(state) > 0 else 'lightcoral'\n",
    "                    rect = patches.Rectangle((j, i), 1, 1, \n",
    "                                            facecolor=color, alpha=0.7,\n",
    "                                            edgecolor='black', linewidth=2)\n",
    "                    ax.add_patch(rect)\n",
    "                    label = 'G' if self.mdp.get_reward(state) > 0 else 'T'\n",
    "                    ax.text(j + 0.5, i + 0.5, label, \n",
    "                           ha='center', va='center', \n",
    "                           fontsize=14, fontweight='bold')\n",
    "                \n",
    "                # Obstacles\n",
    "                elif state in self.mdp.obstacles:\n",
    "                    rect = patches.Rectangle((j, i), 1, 1,\n",
    "                                            facecolor='gray', alpha=0.6,\n",
    "                                            edgecolor='black', linewidth=1)\n",
    "                    ax.add_patch(rect)\n",
    "                    ax.text(j + 0.5, i + 0.5, '⛔',\n",
    "                           ha='center', va='center', fontsize=16)\n",
    "                \n",
    "                # Regular states with policy\n",
    "                elif state in policy and policy[state]:\n",
    "                    action = policy[state]\n",
    "                    ax.text(j + 0.5, i + 0.5, action,\n",
    "                           ha='center', va='center',\n",
    "                           fontsize=20, fontweight='bold', color='blue')\n",
    "        \n",
    "        ax.set_xticks(range(self.mdp.cols + 1))\n",
    "        ax.set_yticks(range(self.mdp.rows + 1))\n",
    "        ax.grid(True)\n",
    "    \n",
    "    def draw_value_function(self, ax, V, title):\n",
    "        \"\"\"Draw value function as heatmap\"\"\"\n",
    "        ax.set_title(title, fontsize=11, fontweight='bold')\n",
    "        \n",
    "        # Create value matrix\n",
    "        value_matrix = np.zeros((self.mdp.rows, self.mdp.cols))\n",
    "        for i in range(self.mdp.rows):\n",
    "            for j in range(self.mdp.cols):\n",
    "                state = (i, j)\n",
    "                if state in V:\n",
    "                    value_matrix[i, j] = V[state]\n",
    "                else:\n",
    "                    value_matrix[i, j] = np.nan\n",
    "        \n",
    "        # Create custom colormap\n",
    "        cmap = LinearSegmentedColormap.from_list('custom', \n",
    "                                                 ['red', 'yellow', 'green'])\n",
    "        \n",
    "        # Plot heatmap\n",
    "        im = ax.imshow(value_matrix, cmap=cmap, aspect='auto')\n",
    "        \n",
    "        # Add value text\n",
    "        for i in range(self.mdp.rows):\n",
    "            for j in range(self.mdp.cols):\n",
    "                state = (i, j)\n",
    "                if state in V:\n",
    "                    text = ax.text(j, i, f'{V[state]:.2f}',\n",
    "                                 ha='center', va='center',\n",
    "                                 fontsize=9, fontweight='bold')\n",
    "                    \n",
    "                    # Add background for readability\n",
    "                    if state in self.mdp.obstacles:\n",
    "                        ax.add_patch(patches.Rectangle((j-0.5, i-0.5), 1, 1,\n",
    "                                                       facecolor='gray', alpha=0.3))\n",
    "        \n",
    "        # Colorbar\n",
    "        plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "        \n",
    "        ax.set_xticks(range(self.mdp.cols))\n",
    "        ax.set_yticks(range(self.mdp.rows))\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    def draw_statistics(self, ax):\n",
    "        \"\"\"Draw statistics and parameters\"\"\"\n",
    "        ax.axis('off')\n",
    "        \n",
    "        stats_text = f\"\"\"\n",
    "POLICY ITERATION RESULTS\n",
    "{'='*40}\n",
    "\n",
    "Total Iterations: {self.policy_iter.improvement_count}\n",
    "\n",
    "Evaluation Iterations per Step:\n",
    "{', '.join(map(str, self.policy_iter.eval_iterations[:5]))}\n",
    "{'...' if len(self.policy_iter.eval_iterations) > 5 else ''}\n",
    "\n",
    "Average V per Iteration:\n",
    "\"\"\"\n",
    "        \n",
    "        for i, V in enumerate(self.policy_iter.value_history[:5]):\n",
    "            avg_v = np.mean([v for v in V.values()])\n",
    "            stats_text += f\"  Iter {i+1}: {avg_v:.3f}\\n\"\n",
    "        \n",
    "        stats_text += f\"\"\"\n",
    "{'='*40}\n",
    "PARAMETERS\n",
    "\n",
    "Discount Factor (γ): {self.policy_iter.gamma}\n",
    "Convergence Threshold (θ): {self.policy_iter.theta}\n",
    "Transition Probability: {self.mdp.transition_prob}\n",
    "\n",
    "{'='*40}\n",
    "LEGEND\n",
    "\n",
    "→ ↓ ← ↑  : Policy actions\n",
    "G        : Goal (terminal)\n",
    "T        : Terminal state\n",
    "⛔       : Obstacle\n",
    "\"\"\"\n",
    "        \n",
    "        ax.text(0.05, 0.95, stats_text, transform=ax.transAxes,\n",
    "               fontsize=9, verticalalignment='top',\n",
    "               fontfamily='monospace',\n",
    "               bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16cd0458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfc85aeb1c8a4e7da1cd9b22a336e8c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Environment:', options=(('Grid World (4×4)', 'grid'), ('Cliff Walking (4×…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Assuming these classes already exist:\n",
    "# MDP, PolicyIteration, PolicyIterationVisualizer\n",
    "\n",
    "def run_policy_iteration(env_type, gamma):\n",
    "    clear_output(wait=True)\n",
    "    print(\"=\"*70)\n",
    "    print(\"=\"*70)\n",
    "    print(\"Solving MDP using Dynamic Programming.\\n\")\n",
    "    print(\"This program implements Policy Evaluation and Improvement.\\n\")\n",
    "\n",
    "    print(f\"Initializing {env_type} MDP...\")\n",
    "    mdp = MDP(env_type)\n",
    "    print(f\"MDP Size: {mdp.rows}x{mdp.cols}\")\n",
    "    print(f\"States: {len(mdp.states)}\")\n",
    "    print(f\"Actions: {mdp.actions}\")\n",
    "    print(f\"Transition Probability: {mdp.transition_prob}\")\n",
    "\n",
    "    print(\"\\nStarting Policy Iteration...\")\n",
    "    policy_iter = PolicyIteration(mdp, gamma=gamma)\n",
    "    optimal_policy, optimal_values = policy_iter.iterate()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"POLICY ITERATION COMPLETE!\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    print(\"\\nSample Optimal Values:\")\n",
    "    for i, (state, value) in enumerate(list(optimal_values.items())[:10]):\n",
    "        print(f\"  V({state}) = {value:.4f}\")\n",
    "\n",
    "    print(\"\\nSample Optimal Policy:\")\n",
    "    policy_items = [(s, a) for s, a in optimal_policy.items() if a is not None]\n",
    "    for i, (state, action) in enumerate(policy_items[:10]):\n",
    "        print(f\"  π({state}) = {action}\")\n",
    "\n",
    "    print(\"\\nGenerating visualizations...\")\n",
    "    visualizer = PolicyIterationVisualizer(mdp, policy_iter)\n",
    "    visualizer.visualize_results()\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Widgets\n",
    "# --------------------------\n",
    "\n",
    "env_dropdown = widgets.Dropdown(\n",
    "    options=[(\"Grid World (4×4)\", \"grid\"),\n",
    "             (\"Cliff Walking (4×12)\", \"cliff\"),\n",
    "             (\"Stochastic Grid (5×5)\", \"stochastic\")],\n",
    "    value=\"grid\",\n",
    "    description=\"Environment:\"\n",
    ")\n",
    "\n",
    "gamma_slider = widgets.FloatSlider(\n",
    "    value=0.9,\n",
    "    min=0.0,\n",
    "    max=0.99,\n",
    "    step=0.01,\n",
    "    description=\"Gamma (γ):\",\n",
    "    readout_format=\".2f\"\n",
    ")\n",
    "\n",
    "run_button = widgets.Button(\n",
    "    description=\"Run Policy Iteration\",\n",
    "    button_style=\"success\"\n",
    ")\n",
    "\n",
    "def on_button_click(_):\n",
    "    run_policy_iteration(env_dropdown.value, gamma_slider.value)\n",
    "\n",
    "run_button.on_click(on_button_click)\n",
    "\n",
    "# Display UI\n",
    "dashboard = widgets.VBox([env_dropdown, gamma_slider, run_button])\n",
    "display(dashboard)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
